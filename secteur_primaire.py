# -*- coding: utf-8 -*-
"""Secteur Primaire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QQPQony4fZj1WzH61qwqAa6FOwd-8Gwf
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from math import sqrt
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
from math import sqrt
import seaborn as sb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import  LogisticRegression
from sklearn.metrics import precision_recall_fscore_support as score
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
from scipy.stats import mstats

# Load the dataset
file_path = '/content/Produit intérieur brut trimestriel (base 2014).xlsx'
data = pd.read_excel(file_path)

# Display the first few rows and general information about the dataset to understand its structure
data.head()
#data.info()

# Drop columns that may not be useful for analysis
data_cleaned = data.drop(columns=["Units", "Scale"])

# Transpose data so each sector can be used as a feature, with time indices as rows
data_transposed = data_cleaned.set_index("Variables").T

# Rename columns for clarity and reset index
data_transposed = data_transposed.reset_index()
data_transposed.columns.name = None

# Check if "PIB total" is in the data (this would be our target variable)
target_variable = "Produit intérieur brut (PIB)"
#data_transposed.head()
target_variable in data_transposed.columns

# Rename the target column for clarity
data_transposed.rename(columns={"PRODUIT INTERIEUR BRUT (en milliards de FCFA)": "PIB_total"}, inplace=True)
data_transposed.rename(columns={"VALEUR AJOUTEE (VA) DU SECTEUR PRIMAIRE (en milliards de FCFA)": "PRIMAIRE"}, inplace=True)
data_transposed.rename(columns={"VALEUR AJOUTEE (VA) DU SECTEUR SECONDAIRE (en milliards de FCFA)": "SECONDAIRE"}, inplace=True)
data_transposed.rename(columns={"VALEUR AJOUTEE (VA) DU SECTEUR TERTIAIRE (en milliards de FCFA)": "TERTIAIRE"}, inplace=True)
data_transposed.rename(columns={"AGRICULTURE ET ACTIVITES ANNEXES": "AGRICULTURE"}, inplace=True)
data_transposed.rename(columns={"ELEVAGE ET CHASSE": "ELEV_CHAS"}, inplace=True)
data_transposed.rename(columns={"SYLVICULTURE, EXPLOITATION FORESTIÈRE ET ACTIVITÉS DE SOUTIEN": "Foresterie"}, inplace=True)
data_transposed.rename(columns={"PECHE, AQUACULTURE ET PISCICULTURE": "Aquaculture"}, inplace=True)
data_transposed.rename(columns={"ACTIVITES EXTRACTIVES": "Extraction"}, inplace=True)
data_transposed.rename(columns={"FABRICATION DE PRODUITS AGRO-ALIMENTAIRES": "AGRO-ALIMENTAIRES"}, inplace=True)
data_transposed.rename(columns={"RAFINAGE DU PETROLE ET COKEFACTION": "Raffinage"}, inplace=True)
data_transposed.rename(columns={"FABRICATION DE PRODUITS CHIMIQUES DE BASE": "PRODUITS CHIMIQUES"}, inplace=True)
data_transposed.rename(columns={"FABRICATION DE CIMENT ET D'AUTRES MATERIAUX DE CONSTRUCTION": "CIMENT_MATERIAUX_AUTRES"}, inplace=True)
data_transposed.rename(columns={"FABRICATION D'AUTRES PRODUITS MANUFACTURIERS": "MANUFACTURIERS_AUTRES"}, inplace=True)
data_transposed.rename(columns={"PRODUCTION ET DISTRIBUTION D'ÉLECTRICITÉ ET DE GAZ": "ÉLECTRICITÉ_GAZ"}, inplace=True)
data_transposed.rename(columns={"PRODUCTION ET DISTRIBUTION D'EAU, ASSAINISSEMENT ET TRAITEMENT DES DECHETS": "Hydraulique"}, inplace=True)
data_transposed.rename(columns={"CONSTRUCTION": "CONSTRUCTION"}, inplace=True)
data_transposed.rename(columns={"COMMERCE": "COMMERCE"}, inplace=True)
data_transposed.rename(columns={"HEBERGEMENT ET RESTAURATION": "Hospitalité"}, inplace=True)
data_transposed.rename(columns={"INFORMATION ET COMMUNICATION": "Médiation"}, inplace=True)
data_transposed.rename(columns={"ACTIVITES FINANCIERES ET D'ASSURANCE": "FINANCE"}, inplace=True)
data_transposed.rename(columns={"ACTIVITES  IMMOBILIERES": "IMMOBILIERES"}, inplace=True)
data_transposed.rename(columns={"SERVICES AUX ENTREPRISES": "ENTREPRISES"}, inplace=True)
data_transposed.rename(columns={"ACTIVITES D'ADMINISTRATION PUBLIQUE, D'ENSEIGNEMENT ET DE SANTE": "ADMINISTRATION"}, inplace=True)
data_transposed.rename(columns={"ACTIVITES DOMESTIQUES": "DOMESTIQUES"}, inplace=True)
data_transposed.rename(columns={"AUTRES ACTIVITES DE SERVICES": "SERVICES_AUTRES"}, inplace=True)
data_transposed.rename(columns={"TAXES NETTES SUR LES PRODUITS": "TAXES"}, inplace=True)

data_secteur=data_transposed.drop(columns=["index"])
data_secteur.head()

data1=data_secteur[['AGRICULTURE','ELEV_CHAS', 'Foresterie','Aquaculture','PIB_total']]



data1.head()

import seaborn as sns

# Tracé des boxplots pour chaque variable
for column in data1.columns:
    fig, ax = plt.subplots(figsize=(8, 3))
    sns.boxplot(x=data1[column], ax=ax)
    plt.title(column)
    plt.show()



"""**Detection de valeurs manquantes**

Agricultures,

Aquaculture


"""

#data1['PRIMAIRES']=mstats.winsorize(data1['PRIMAIRE'],
#                                      limits=[0.05,0.05])
data1['AGRICULTURES']=mstats.winsorize(data1['AGRICULTURE'],
                                       limits=[0.05,0.05])
data1['Aquacultures']=mstats.winsorize(data1['Aquaculture'],
                                      limits=[0.05,0.05])

df=data1.drop(columns=['Aquaculture','AGRICULTURE'])

# Tracé des boxplots pour chaque variable
for column in df.columns:
    fig, ax = plt.subplots(figsize=(8, 3))
    sns.boxplot(x=df[column], ax=ax)
    plt.title(column)
    plt.show()

"""**Stats descriptives**"""

df.describe()

"""**Normalisation des variables**"""

def normalization(dataToNormalize):
    columns = dataToNormalize.columns
    for col in columns:
        x = dataToNormalize[[col]].values.astype(float)
        standard_normalization = preprocessing.StandardScaler()
        res = standard_normalization.fit_transform(x)
        dataToNormalize[col]=res

normalization(df)

df.describe()

corr_matrix=df.corr()
sb.heatmap(corr_matrix,annot=True,cmap='RdYlGn',linewidths=0.2)
fig=plt.gcf()
fig.set_size_inches(9,8)
plt.show()

y=df['PIB_total']

data=df.drop(columns=['PIB_total'])

x_train, x_test, y_train, y_test = train_test_split(data, y,
test_size=0.2)

regression_alg = LinearRegression()

regression_alg.fit(x_train, y_train)

# Fit du modèle linéaire
import statsmodels.api as sm
lr_model = sm.OLS(y_train, sm.add_constant(x_train)).fit()

# Obtention du tableau de résultats
results_table = lr_model.summary2().tables[1]
results_table

# Obtention des prédictions et des métriques d'évaluation
import numpy as np
y_train_pred = lr_model.predict(sm.add_constant(x_train))
y_test_pred = lr_model.predict(sm.add_constant(x_test))
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_train = np.sqrt(mse_train)
rmse_test = np.sqrt(mse_test)

# Construction du tableau de résultats

df_metrics = pd.DataFrame({
    'Métrique': ['R2', 'MSE', 'RMSE'],
    'Train': [r2_train, mse_train, rmse_train],
    'Test': [r2_test, mse_test, rmse_test]
})

print(df_metrics)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# ... (Your previous code for model training and predictions)

# Tracer les valeurs observées en fonction des valeurs prédites sur les données d'entraînement
plt.figure(figsize=(8, 6))

# Ensure both y_train and y_train_pred are NumPy arrays or have the same index if they are Pandas Series
y_train_values = y_train.values if isinstance(y_train, pd.Series) else y_train
y_train_pred_values = y_train_pred.values if isinstance(y_train_pred, pd.Series) else y_train_pred

# Check and fix potential length mismatch caused by index differences
if len(y_train_values) != len(y_train_pred_values):
    # Attempt to align indices if y_train and y_train_pred are pandas Series
    if isinstance(y_train, pd.Series) and isinstance(y_train_pred, pd.Series):
        common_index = y_train.index.intersection(y_train_pred.index)
        y_train_values = y_train.loc[common_index].values
        y_train_pred_values = y_train_pred.loc[common_index].values
    else:
        # Handle other cases where the length mismatch occurs (add specific logic here)
        raise ValueError("y_train and y_train_pred have different lengths and cannot be aligned.")

plt.scatter(y_train_values, y_train_pred_values, color='blue') # Use aligned values for plotting
plt.xlabel('Observed values')
plt.ylabel('Predicted values')
plt.title('Training data')
plt.plot(np.arange(-1, 3), np.arange(-1, 3), color='red')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns  # Import seaborn

# ... (Your previous code for model training and predictions)

# Test d'homoscédasticité
y_train_pred = lr_model.predict(sm.add_constant(x_train))
residuals = y_train - y_train_pred

plt.figure(figsize=(8, 6))

# Convert y_train_pred and residuals to NumPy arrays or list if they are Pandas Series
y_train_pred_values = y_train_pred.values if isinstance(y_train_pred, pd.Series) else y_train_pred
residuals_values = residuals.values if isinstance(residuals, pd.Series) else residuals

# Use scatterplot with 'x' and 'y' arguments for clarity
sns.scatterplot(x=y_train_pred_values, y=residuals_values)

plt.title('Test d\'homoscédasticité')
plt.xlabel('Prédictions')
plt.ylabel('Résidus')
plt.show()

# Test de Breusch-Pagan
bp_test = sm.stats.diagnostic.het_breuschpagan(lr_model.resid, lr_model.model.exog)
print("Test de Breusch-Pagan : statistique = %f, p-value = %f" % (bp_test[0], bp_test[1]))

# Test de multicollinéarité
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])]
vif["features"] = x_train.columns
print(vif)

# Test de normalité
resid_norm = lr_model.get_influence().resid_studentized_internal
plt.figure(figsize=(8, 6))
sns.histplot(resid_norm, kde=True)
plt.title('Test de normalité')
plt.xlabel('Résidus normalisés')
plt.ylabel('Fréquence')
plt.show()

# QQ-plot
fig, ax = plt.subplots(figsize=(8, 6))
sm.qqplot(residuals, line='s', ax=ax)
ax.set_title('QQ-plot')
plt.show()

import scipy.stats as stats

# Test de Shapiro-Wilk
shapiro_test = stats.shapiro(residuals)

# Test d'Anderson-Darling
anderson_test = stats.anderson(residuals, dist='norm')

# Test de Kolmogorov-Smirnov
ks_test = stats.kstest(residuals, 'norm')

# Mettre les résultats dans un dataframe
df_normality = pd.DataFrame({
    'Test de normalité': ['Shapiro-Wilk', 'Anderson-Darling', 'Kolmogorov-Smirnov'],
    'Statistique': [shapiro_test.statistic, anderson_test.statistic, ks_test.statistic],
    'p-value': [shapiro_test.pvalue, 'N/A', ks_test.pvalue]
})

# Afficher le tableau
print(df_normality)

# Afficher les résultats dans un tableau
results_table = lr_model.summary()
print(results_table)

################################################

train_predictions = regression_alg.predict(x_train)

print(f"RMSE = {round(sqrt(mean_squared_error(y_train,train_predictions)),2)}")

print(f"R2_score = {round(r2_score(y_train,train_predictions),2)}")

test_predictions = regression_alg.predict(x_test)

print(f"RMSE = {round(sqrt(mean_squared_error(y_test,test_predictions)),2)}")

print(f"R2_score = {round(r2_score(y_test,test_predictions),2)}")

plt.scatter(y_test, test_predictions, color='black')
plt.title("les prédictions du modèle vs la réalité")
plt.xlabel("les valeurs observées")
plt.ylabel("Les prédictions")
plt.plot([00.0, 15.0], [00.0, 15.0], 'red', lw=1)
plt.show()

def average_result(nb_run):
    average_rmse = 0
    average_r2 = 0
    for i_run in range(nb_run):
        x_train, x_test, y_train, y_test = train_test_split(data,
        y, test_size=0.2)

        regression_alg = LinearRegression()
        regression_alg.fit(x_train, y_train)

        test_predictions = regression_alg.predict(x_test)

        i_run_rmse = sqrt(mean_squared_error(y_test,
                          test_predictions))
        i_run_r2 = r2_score(y_test, test_predictions)

        print(f"Run {i_run} : RMSE = {round(i_run_rmse,2)}-R2_score = {round(i_run_r2,2)}")

        average_rmse = average_rmse + i_run_rmse
        average_r2 = average_r2 + i_run_r2

    average_rmse = average_rmse / nb_run
    average_r2 = average_r2 / nb_run

    print(f"Moyenne : RMSE = {round(average_rmse,2)} - R2_score = {round(average_r2,2)}")

average_result(10)

from sklearn.model_selection import KFold

kf = KFold(n_splits=2, shuffle=False)

for train_index, test_index in kf.split(data):
    print("Les indices de train_index = ", train_index)
    print("Les indices de test_index = ", train_index)
    print("\n\n")

kf = KFold(n_splits=3, shuffle=False)
for train_index, test_index in kf.split(data):
    print(f"Le nombre d'éléments dans train_index ={train_index.shape[0]}")
    print(f"Le nombre d'éléments dans test_index ={test_index.shape[0]}\n")

def create_evaluate_model(index_fold, x_train, x_test, y_train,
y_test):
    regression_alg = LinearRegression()
    regression_alg.fit(x_train, y_train)
    test_predictions = regression_alg.predict(x_test)
    rmse = sqrt(mean_squared_error(y_test, test_predictions))
    r2 = r2_score(y_test, test_predictions)
    print(f"Run {index_fold} : RMSE = {round(rmse,2)} - R2_score = {round(r2,2)}")
    return (rmse, r2)

nb_model = 5
kf = KFold(n_splits=nb_model, shuffle=False)

index_fold = 0
average_rmse = 0
average_r2 = 0

for train_index, test_index in kf.split(data):
  x_train, x_test = data.iloc[train_index],data.iloc[test_index]
  # Use .iloc to select rows based on integer position instead of index labels
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]
  current_rmse, current_r2 = create_evaluate_model(index_fold,
                              x_train, x_test, y_train, y_test)
  average_rmse = average_rmse + current_rmse
  average_r2 = average_r2 + current_r2
  index_fold = index_fold + 1
  average_rmse = average_rmse / nb_model
  average_r2 = average_r2 / nb_model

print(f"Moyenne : RMSE = {round(average_rmse,2)} - R2_score ={round(average_r2,2)}")

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 1. Tickers des 10 plus grosses capitalisations du CAC 40 (exemple, à ajuster selon ta liste exacte)
tickers = ['MC.PA', 'OR.PA', 'TTE.PA', 'EL.PA', 'AI.PA', 'SAN.PA', 'SU.PA', 'SAF.PA', 'BNP.PA', 'DG.PA']

# 2. Récupération des prix sur 1 an
data = yf.download(tickers, start="2024-07-01", end="2025-07-17")['Close']
returns = data.pct_change().dropna()

# 3. Statistiques
mean_returns = returns.mean()  # Rendement espéré
cov_matrix = returns.cov()     # Matrice de covariance

# 4. Paramètres de simulation
num_portfolios = 10000
results = np.zeros((3, num_portfolios))
weights_record = []

# 5. Simulation Monte Carlo
np.random.seed(42)
for i in range(num_portfolios):
    weights = np.random.random(len(tickers))
    weights /= np.sum(weights)

    portfolio_return = np.dot(weights, mean_returns)
    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    sharpe_ratio = portfolio_return / portfolio_std

    results[0, i] = portfolio_return
    results[1, i] = portfolio_std
    results[2, i] = sharpe_ratio
    weights_record.append(weights)

# 6. Portefeuille optimal
results_df = pd.DataFrame(results.T, columns=['Rendement', 'Volatilité', 'Sharpe'])
max_sharpe_idx = results_df['Sharpe'].idxmax()
opt_weights = weights_record[max_sharpe_idx]

allocation = pd.DataFrame({'Ticker': tickers, 'Poids optimal': np.round(opt_weights, 4)})
allocation['Investissement (€)'] = allocation['Poids optimal'] * 500_000

# Affichage
print("✅ Allocation optimale :")
print(allocation)

# 7. Visualisation
plt.figure(figsize=(10,6))
plt.scatter(results_df['Volatilité'], results_df['Rendement'], c=results_df['Sharpe'], cmap='viridis')
plt.colorbar(label='Ratio de Sharpe')
plt.scatter(results_df.loc[max_sharpe_idx]['Volatilité'], results_df.loc[max_sharpe_idx]['Rendement'],
            color='r', marker='*', s=200, label='Portefeuille optimal')
plt.title('Frontière efficiente (10 plus grosses valeurs CAC 40)')
plt.xlabel('Volatilité')
plt.ylabel('Rendement espéré')
plt.legend()
plt.grid(True)
plt.show()

results_df

results

import yfinance as yf
import pandas as pd
import numpy as np

import numpy as np
import pandas as pd

# Données
nominal = 100_000_000  # 100 millions €
maturite = 3  # ans

# Taux fixes
taux_fixe = 0.0275

# Taux variable (courbe japonaise)
courbe_japonaise = np.array([0.015, 0.018, 0.020])
spread = -0.0075  # -75bp

# Courbe de discount américaine (taux d'actualisation)
courbe_discount = np.array([0.02, 0.021, 0.022])

# Calcul des cash-flows fixes
cf_fixes = np.full(maturite, nominal * taux_fixe)

# Calcul des cash-flows variables
taux_variables = courbe_japonaise + spread
cf_variables = nominal * taux_variables

# Calcul des facteurs d'actualisation
facteurs_discount = 1 / (1 + courbe_discount) ** np.arange(1, maturite + 1)

# Actualisation des cash-flows
va_fixes = np.sum(cf_fixes * facteurs_discount)
va_variables = np.sum(cf_variables * facteurs_discount)

# Valorisation du swap
valeur_swap = va_variables - va_fixes

# Affichage des résultats
df = pd.DataFrame({
    'Année': np.arange(1, maturite + 1),
    'CF Fixes (M€)': cf_fixes / 1e6,
    'CF Variables (M€)': cf_variables / 1e6,
    'Facteur discount': facteurs_discount,
    'VA CF Fixes (M€)': cf_fixes * facteurs_discount / 1e6,
    'VA CF Variables (M€)': cf_variables * facteurs_discount / 1e6
})

print(df)
print(f"\nValeur du swap (en millions €) : {valeur_swap / 1e6:.3f}")

df_results = pd.DataFrame(results)

# Vérifier les colonnes et types
print(df_results.columns)
print(df_results.dtypes)

# Forcer la conversion en numérique pour Sharpe et supprimer les lignes invalides
df_results["Sharpe"] = pd.to_numeric(df_results["Sharpe"], errors='coerce')
df_results = df_results.dropna(subset=["Sharpe"])

# Trier par Sharpe
df_results.sort_values("Sharpe", ascending=False, inplace=True)

print(df_results)

import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt

# Liste des tickers des taux américains par maturité
# Source approximative des US Treasury yields
tickers = {
    '1M': '^IRX',
    '1Y': '^FVX',
    '10Y': '^TNX',
    '30Y': '^TYX'
}

data = {}

# Récupération des taux
for label, ticker in tickers.items():
    taux = yf.Ticker(ticker).history(period='1d')  # dernier jour
    if not taux.empty:
        dernier = taux['Close'].iloc[-1] / 100  # Yahoo les donne en %
        data[label] = dernier

# Conversion en DataFrame
df_taux = pd.DataFrame.from_dict(data, orient='index', columns=['Taux'])
df_taux.index.name = 'Maturité'
df_taux = df_taux.sort_index()

# Affichage
print(df_taux)

# Graphique de la courbe
df_taux.plot(marker='o', legend=False, title="Courbe de taux US (approximation)")
plt.ylabel("Taux")
plt.grid()
plt.show()

import requests
from bs4 import BeautifulSoup

def get_yield_curve(country_code, label_contains):
    """
    Récupère les taux souverains depuis Boursorama pour un pays donné.

    :param country_code: "JPN" pour Japon, "USA" pour USA
    :param label_contains: substring pour détecter les lignes (ex: "JPN BENCHMARK", "USA BENCHMARK")
    """
    url = (
        "https://www.boursorama.com/bourse/taux/souverains/"
        f"?area_filter%5Barea%5D=world&area_filter%5Bcountries%5D={country_code}"
    )
    resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(resp.text, 'html.parser')

    table = soup.find("table")
    if table is None:
        raise ValueError(f"Aucun tableau trouvé pour {country_code}")

    curve = {}
    for row in table.find_all("tr"):
        cols = row.find_all("td")
        if len(cols) >= 2:
            label = cols[0].get_text(strip=True)
            taux_text = cols[1].get_text(strip=True).replace('%', '').replace(',', '.')
            if label_contains in label and taux_text:
                try:
                    maturity = int(label.split()[-1].replace("A", ""))
                    taux = float(taux_text) / 100
                    curve[maturity] = taux
                except ValueError:
                    continue

    return curve

# Récupération des deux courbes
japan_curve = get_yield_curve("JPN", "JPN BENCHMARK")
usa_curve = get_yield_curve("USA", "USA BENCHMARK")

# Affichage
print("🇯🇵 Courbe japonaise :")
for m, t in sorted(japan_curve.items()):
    print(f" {m} ans : {t:.4%}")

print("\n🇺🇸 Courbe américaine :")
for m, t in sorted(usa_curve.items()):
    print(f" {m} ans : {t:.4%}")

import numpy as np
from scipy.interpolate import interp1d

def interpolate_curve(curve_dict, min_year=1, max_year=15):
    """Interpoler linéairement une courbe de taux entre min_year et max_year"""
    known_maturities = np.array(sorted(curve_dict.keys()))
    known_rates = np.array([curve_dict[m] for m in known_maturities])

    interpolator = interp1d(known_maturities, known_rates, kind='linear', fill_value='extrapolate')

    full_curve = {}
    for year in range(min_year, max_year + 1):
        full_curve[year] = float(interpolator(year))

    return full_curve


# Interpolation des deux courbes
#interpolated_japan = interpolate_curve(japan_curve, 1, 20)
interpolated_usa = interpolate_curve(usa_curve, 1, 20)

# Affichage
#print("🇯🇵 Courbe japonaise interpolée (1 à 15 ans) :")
#for y in range(1, 21):
#    print(f" {y} ans : {interpolated_japan[y]:.4%}")

print("\n🇺🇸 Courbe américaine interpolée (1 à 20 ans) :")
for y in range(1, 21):
    print(f" {y} ans : {interpolated_usa[y]:.4%}")

import numpy as np
from scipy.interpolate import interp1d

def interpolate_curve(curve_dict, min_year=1, max_year=15):
    """Interpoler linéairement une courbe de taux entre min_year et max_year"""
    known_maturities = np.array(sorted(curve_dict.keys()))
    known_rates = np.array([curve_dict[m] for m in known_maturities])

    interpolator = interp1d(known_maturities, known_rates, kind='linear', fill_value='extrapolate')

    full_curve = {}
    for year in range(min_year, max_year + 1):
        full_curve[year] = float(interpolator(year))

    return full_curve

# Interpolation des deux courbes
interpolated_japan = interpolate_curve(japan_curve, 1, 20)
#interpolated_usa = interpolate_curve(usa_curve, 1, 20)

# Affichage
print("🇯🇵 Courbe japonaise interpolée (1 à 20 ans) :")
for y in range(1, 21):
    print(f" {y} ans : {interpolated_japan[y]:.4%}")

#print("\n🇺🇸 Courbe américaine interpolée (1 à 20 ans) :")
#for y in range(1, 21):
#    print(f" {y} ans : {interpolated_usa[y]:.4%}")

from math import pow

# Paramètres du swap
notional = 100
fixed_rate = 0.0275
spread = 0.0075
maturity_years = 15

print("=== Calcul des cash-flows fixes ===")
cash_flows_fixed = []
for year in range(1, maturity_years + 1):
    cf_fix = notional * fixed_rate
    cash_flows_fixed.append(cf_fix)
    print(f"Année {year}: CF fixe = {cf_fix:.4f}")

print("\n=== Calcul des cash-flows variables ===")
cash_flows_variable = []
for year in range(1, maturity_years + 1):
    taux_var = interpolated_japan[year] - spread
    cf_var = notional * taux_var
    cash_flows_variable.append(cf_var)
    print(f"Année {year}: Taux Japon (net spread) = {taux_var:.4%}, CF variable = {cf_var:.4f}")

print("\n=== Actualisation des flux avec la courbe américaine ===")
discounted_fixed = []
discounted_variable = []
for year in range(1, maturity_years + 1):
    df = 1 / pow(1 + interpolated_usa[year], year)

    cf_fix_disc = cash_flows_fixed[year-1] * df
    cf_var_disc = cash_flows_variable[year-1] * df

    discounted_fixed.append(cf_fix_disc)
    discounted_variable.append(cf_var_disc)

    print(f"Année {year}: DF = {df:.6f}")
    print(f"  CF fixe actualisé = {cf_fix_disc:.4f}")
    print(f"  CF variable actualisé = {cf_var_disc:.4f}")

# Calcul de la valeur du swap
valeur_fixe = sum(discounted_fixed)
valeur_variable = sum(discounted_variable)
valeur_swap = valeur_variable - valeur_fixe

print("\n=== Résultats finaux ===")
print(f"Valeur actuelle totale des flux fixes      : {valeur_fixe:.4f}")
print(f"Valeur actuelle totale des flux variables  : {valeur_variable:.4f}")
print(f"Valeur nette du swap (variable - fixe)     : {valeur_swap:.4f}")

import pandas as pd
from math import pow

# Paramètres du swap
notional = 100
fixed_rate = 0.0275
spread = 0.0075
maturity_years = 15

# Préparer les données pour les taux interpolés (exemple avec tes interpolated_japan et interpolated_usa)
# Construire un DataFrame des taux
years = list(range(1, maturity_years + 1))
taux_japon = [interpolated_japan[y] for y in years]
taux_usa = [interpolated_usa[y] for y in years]

df_taux = pd.DataFrame({
    'Année': years,
    'Taux Japon': taux_japon,
    'Taux USA': taux_usa
})

# Calcul des cash-flows et actualisation
cash_flows_fixed = []
cash_flows_variable = []
discount_factors = []
discounted_fixed = []
discounted_variable = []

for year in years:
    cf_fix = notional * fixed_rate
    taux_var = interpolated_japan[year] - spread
    cf_var = notional * taux_var
    df = 1 / pow(1 + interpolated_usa[year], year)
    cf_fix_disc = cf_fix * df
    cf_var_disc = cf_var * df

    cash_flows_fixed.append(cf_fix)
    cash_flows_variable.append(cf_var)
    discount_factors.append(df)
    discounted_fixed.append(cf_fix_disc)
    discounted_variable.append(cf_var_disc)

valeur_fixe = sum(discounted_fixed)
valeur_variable = sum(discounted_variable)
valeur_swap = valeur_variable - valeur_fixe

# DataFrame détails flux
df_details = pd.DataFrame({
    'Année': years,
    'CF Fixe': cash_flows_fixed,
    'CF Variable': cash_flows_variable,
    'Facteur Actualisation': discount_factors,
    'CF Fixe Actualisé': discounted_fixed,
    'CF Variable Actualisé': discounted_variable,
})

# DataFrame résultats finaux
df_summary = pd.DataFrame({
    'Description': [
        'Valeur actuelle totale des flux fixes',
        'Valeur actuelle totale des flux variables',
        'Valeur nette du swap (variable - fixe)'
    ],
    'Valeur': [valeur_fixe, valeur_variable, valeur_swap]
})

# Export Excel avec 3 feuilles
with pd.ExcelWriter('resultats_swap_3_feuilles.xlsx') as writer:
    df_taux.to_excel(writer, sheet_name='Taux', index=False)
    df_details.to_excel(writer, sheet_name='Détails Flux', index=False)
    df_summary.to_excel(writer, sheet_name='Résultats Finaux', index=False)

print("Fichier Excel 'resultats_swap_3_feuilles.xlsx' créé avec succès !")



"""## Taux pour Japon"""

import pandas as pd

# Assuming interpolated_japan is a dictionary with years as keys and rates as values
df_interpolated_japan = pd.DataFrame.from_dict(interpolated_japan, orient='index', columns=['Taux Interpolé'])
df_interpolated_japan.index.name = 'Année'

# Save to Excel
df_interpolated_japan.to_excel('interpolated_japan.xlsx')

print("Fichier Excel 'interpolated_japan.xlsx' créé avec succès !")

"""##Taux pour USA"""

import pandas as pd

# Assuming interpolated_japan is a dictionary with years as keys and rates as values
df_interpolated_usa = pd.DataFrame.from_dict(interpolated_usa, orient='index', columns=['Taux Interpolé'])
df_interpolated_usa.index.name = 'Année'

# Save to Excel
df_interpolated_usa.to_excel('interpolated_usa.xlsx')

print("Fichier Excel 'interpolated_usa.xlsx' créé avec succès !")

"""## Exercice 4"""

import numpy as np
from math import pow

# === Paramètres de l'obligation ===
nominal = 1_000_000
coupon_rate = 0.0425
maturity = 12
coupon = nominal * coupon_rate

# === Exemple de courbe de taux américaine (interpolée) ===
interpolated_usa = {
    y: 0.0205 + (y - 1) * 0.0005  # Taux de 2.05% à 2.05% + 11*0.0005 = 2.6%
    for y in range(1, maturity + 1)
}

# === Calcul des cash-flows ===
cash_flows = [coupon] * (maturity - 1) + [coupon + nominal]

# === Actualisation des flux ===
discounted_flows = []
for year in range(1, maturity + 1):
    rate = interpolated_usa[year]
    df = 1 / pow(1 + rate, year)
    discounted = cash_flows[year - 1] * df
    discounted_flows.append(discounted)
    print(f"Année {year}: Taux = {rate:.4%}, DF = {df:.6f}, Flux = {cash_flows[year - 1]:,.2f}, Actualisé = {discounted:,.2f}")

# === Valeur de l'obligation ===
valeur_obligation = sum(discounted_flows)
print(f"\n📌 Valeur actuelle de l'obligation : {valeur_obligation:,.2f} USD")

"""## Exercice 6: Les pays"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://www.investing.com/rates-bonds/world-government-bonds"
headers = {"User-Agent": "Mozilla/5.0"}
resp = requests.get(url, headers=headers)
soup = BeautifulSoup(resp.content, "html.parser")

target_maturities = ['1Y', '2Y', '3Y', '4Y', '5Y', '6Y', '10Y', '20Y']
target_countries = [
    "Japan", "India", "China",
    "Australia",
    "Tunisia", "Cameroon", "Cote d'Ivoire", "South Africa",
    "France", "Italy", "U.K.",
    "U.S.", "Canada"
]

data = []
seen_entries = set()

for table in soup.find_all("table"):
    for row in table.find_all("tr")[1:]:
        cols = row.find_all("td")
        if len(cols) < 3:
            continue
        label = cols[1].get_text(strip=True)
        yield_str = cols[2].get_text(strip=True).replace('%', '').replace(',', '.')
        for country in target_countries:
            if label.startswith(country):
                mat = label.replace(country, '').strip()
                if mat in target_maturities:
                    entry_key = (country, mat)
                    if entry_key not in seen_entries:
                        try:
                            taux = float(yield_str)
                            data.append([country, mat, taux])
                            seen_entries.add(entry_key)
                        except ValueError:
                            pass

# Construction du DataFrame
df = pd.DataFrame(data, columns=["Country", "Maturity", "Yield"])
pivot = df.pivot(index="Maturity", columns="Country", values="Yield")

# Assurer l'ordre des maturités
pivot = pivot.reindex(target_maturities)

# Convertir l'index en float pour interpoler correctement
pivot.index = pivot.index.str.replace('Y', '').astype(int)

# Interpolation linéaire par colonne
pivot_interpolated = pivot.interpolate(method='linear', axis=0)

# Remettre les labels des maturités
pivot_interpolated.index = [f"{i}Y" for i in pivot_interpolated.index]

# Export
pivot_interpolated.to_excel("Taux_spot_multinationaux.xlsx")

print(pivot_interpolated)

import pandas as pd
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Exemple de données (similaire à ton fichier CSV)
data = {
    'client_id': ['C001', 'C002', 'C001', 'C003', 'C004', 'C002', 'C003', 'C005', 'C001', 'C004'],
    'date_achat': ['2025-07-10', '2025-07-12', '2025-07-20', '2025-07-15', '2025-06-30', '2025-07-22', '2025-07-25', '2025-07-01', '2025-07-30', '2025-07-28'],
    'montant': [120.50, 75.00, 50.00, 200.00, 30.00, 100.00, 150.00, 80.00, 60.00, 45.00]
}

# Création du DataFrame
df = pd.DataFrame(data)
df['date_achat'] = pd.to_datetime(df['date_achat'])

# Date de référence pour calculer la récence (ex : date d’analyse)
date_ref = datetime.strptime('2025-08-01', '%Y-%m-%d')

# Calcul RFM

rfm = df.groupby('client_id').agg({
    'date_achat': lambda x: (date_ref - x.max()).days,  # Récence : jours depuis le dernier achat
    'client_id': 'count',                               # Fréquence : nombre d’achats
    'montant': 'sum'                                    # Montant : somme des dépenses
}).rename(columns={
    'date_achat': 'recence',
    'client_id': 'frequence',
    'montant': 'montant'
}).reset_index()

print("Table RFM :")
print(rfm)

# Normalisation des variables RFM
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['recence', 'frequence', 'montant']])

# Clustering K-means avec 2 clusters (exemple)
kmeans = KMeans(n_clusters=2, random_state=42)
rfm['cluster'] = kmeans.fit_predict(rfm_scaled)

print("\nSegments RFM avec clusters :")
print(rfm)

import matplotlib.pyplot as plt
import seaborn as sns

# Assure-toi que rfm contient la colonne 'cluster' issue du clustering K-means

plt.figure(figsize=(10,6))
sns.scatterplot(data=rfm, x='recence', y='montant', hue='cluster', palette='Set1', s=100)

plt.title('Segmentation clients selon Récence et Montant')
plt.xlabel('Récence (jours depuis dernier achat)')
plt.ylabel('Montant total dépensé (€)')
plt.gca().invert_xaxis()  # Optionnel : plus récent à gauche

plt.legend(title='Cluster')
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # nécessaire pour la 3D
import seaborn as sns

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

# Couleurs pour les clusters
palette = sns.color_palette("Set1", n_colors=rfm['cluster'].nunique())

for cluster in rfm['cluster'].unique():
    cluster_data = rfm[rfm['cluster'] == cluster]
    ax.scatter(
        cluster_data['recence'],
        cluster_data['frequence'],
        cluster_data['montant'],
        s=60,
        c=[palette[cluster]],
        label=f'Cluster {cluster}',
        edgecolor='k',
        alpha=0.8
    )

ax.set_xlabel('Récence (jours)')
ax.set_ylabel('Fréquence (nombre achats)')
ax.set_zlabel('Montant total (€)')
ax.set_title('Segmentation clients en 3D selon RFM')

plt.legend()
plt.show()

import pandas as pd
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import joblib

# Exemple de données (tu peux modifier pour charger depuis un CSV)
data = {
    'client_id': ['C001', 'C002', 'C001', 'C003', 'C004', 'C002', 'C003', 'C005', 'C001', 'C004'],
    'date_achat': ['2025-07-10', '2025-07-12', '2025-07-20', '2025-07-15', '2025-06-30', '2025-07-22', '2025-07-25', '2025-07-01', '2025-07-30', '2025-07-28'],
    'montant': [120.50, 75.00, 50.00, 200.00, 30.00, 100.00, 150.00, 80.00, 60.00, 45.00]
}

df = pd.DataFrame(data)
df['date_achat'] = pd.to_datetime(df['date_achat'])
date_ref = datetime.strptime('2025-08-01', '%Y-%m-%d')

# Calcul RFM
rfm = df.groupby('client_id').agg({
    'date_achat': lambda x: (date_ref - x.max()).days,
    'client_id': 'count',
    'montant': 'sum'
}).rename(columns={
    'date_achat': 'recence',
    'client_id': 'frequence',
    'montant': 'montant'
}).reset_index()

# Normalisation
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['recence', 'frequence', 'montant']])

# Modèle K-means
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(rfm_scaled)

# Sauvegarder le scaler et le modèle dans un dictionnaire
model_data = {
    'scaler': scaler,
    'kmeans': kmeans,
    'rfm': rfm  # On sauvegarde aussi la table RFM (optionnel)
}

joblib.dump(model_data, 'model_rfm_kmeans.pkl')

print("Modèle sauvegardé sous 'model_rfm_kmeans.pkl'")

import streamlit as st
import pandas as pd
import joblib
import plotly.express as px

st.title("Déploiement modèle RFM + K-means")

# Charger le modèle sauvegardé
model_data = joblib.load('model_rfm_kmeans.pkl')
scaler = model_data['scaler']
kmeans = model_data['kmeans']
rfm = model_data['rfm']

# Permettre de choisir le nombre de clusters (optionnel, sinon fixe)
# Ici, on utilise le modèle sauvegardé donc nombre de clusters fixé

# Appliquer le clustering sur les données RFM normalisées
rfm_scaled = scaler.transform(rfm[['recence', 'frequence', 'montant']])
clusters = kmeans.predict(rfm_scaled)
rfm['cluster'] = clusters

st.subheader("Table RFM avec clusters")
st.dataframe(rfm)

# Visualisation interactive 3D avec Plotly
fig = px.scatter_3d(
    rfm,
    x='recence',
    y='frequence',
    z='montant',
    color='cluster',
    hover_data=['client_id'],
    labels={
        'recence': 'Récence (jours)',
        'frequence': 'Fréquence (nb achats)',
        'montant': 'Montant (€)',
        'cluster': 'Cluster'
    },
    title="Segmentation client RFM + K-means"
)

st.plotly_chart(fig, use_container_width=True)

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit